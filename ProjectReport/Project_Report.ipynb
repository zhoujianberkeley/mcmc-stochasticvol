{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quarter 2: Project\n",
    "\n",
    "The main deliverable for the second quarter of the computational economics and finance class is a project to showcase what you have learned. The goal of this project is for you (potentially in a small group) to produce a piece of work (in the form of a Jupyter notebook) that you would be able to use to showcase the skills that you have learned in this class to potential employers or academic advisors.\n",
    "\n",
    "The relatively loose structure in self-directed projects like this makes them a bit more challenging than other things that you will do in school, but we think it also makes them more interesting. They give you a chance to indulge your curiosity and show off your creativity.\n",
    "\n",
    "We have broken the project into three components to keep you on track. Each of these components should be turned in as its own Jupyter notebook. The first two steps are graded almost entirely on whether you do them or not. You must complete the first two steps on your own. When you actually begin working on the final project, you may work in groups of two to four, but you may also work alone if you’d  prefer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Outline\n",
    "\n",
    "- [Project Ideas](#Project-Ideas)\n",
    "- [Proposal](#Proposal)\n",
    "  - [Project Proposals](#Project-Proposals)\n",
    "- [Introduction](#Introduction)\n",
    "  - [Data of VIX Index](#Data-of-VIX-Index)\n",
    "  - [Data with covid-19](#Data-with-covid-19)\n",
    "- [Frequentist Model](#Frequentis-Model)\n",
    "  - [Fitting SP500 returns by ARCH model](#1)\n",
    "  - [Fitting US VIX by ARX model](#2)\n",
    "- [From Random Walk to Free Scale - The Evolution of Stochastic Volatility Model](#Topic-3)\n",
    "  - [Baseline Model: RW Stochastic Volatility](#1)\n",
    "  - [Model Extension: AR1 Stochastic Volatility](#2)\n",
    "  - [Model Extension: Two-State AR1 Stochastic Volatility](#1)\n",
    "  - [Model Extension: Free-Scale Stochastic Volatility](#2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Project Ideas\n",
    "\n",
    "**(15% of project grade)**\n",
    "\n",
    "Form a team of between one and four students (no more) and choose a project. This project could come from the ideas that you submitted or some other idea if you get a sudden flash of inspiration. Flesh out the project in detail -- When we say “flesh it out in detail”, we mean properly load the data into the notebook, describe in words what you want to explore, and create a couple draft quality visualizations (don’t worry about making them pretty) that whets a reader’s appetite.\n",
    "\n",
    "Please include the names of all group members in the below:\n",
    "\n",
    "* Jian Zhou\n",
    "* Bo Sun\n",
    "* Man Chen\n",
    "\n",
    "\n",
    "**Please note that each person should make a copy of the notebook and turn it in!**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proposal\n",
    "Include your project proposal, data, and graphs in the cells below"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Project Proposals"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The stochastic volatility model is the canonical method to model asset return volatility. Nonetheless, the stochastic models usually don’t have closed-form analytic solution and computational-heavy to calibrate due to model complexity. Monte Carlo Markov Chain has grown to be one of the most effective and popular tools in analyzing the stochastic volatility model. Priors and data are fed into the model, and posteriors are the calibrated model parameters of interest. \n",
    "\n",
    "We have noticed that the VIX index has been rather volatile since the outbreak of COVID-19 and peaked in March 2019. The traditional stochastic volatility fails to capture the spike due to the inactivity to internalize exogenous shock.  \n",
    "\n",
    "We proposed to extend the canonical stochastic volatility to incorporate covid-19 data under a Monte Carlo Markov Chain framework. The following formulas set the extended stochastic volatility model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mathematically speaking, the cannonical stochastic volatility model follows the below setting, where $\\sigma_\\eta$ is the scale parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{align*}\n",
    "\\qquad & y_t = \\beta e^{h_t} \\epsilon_t\\qquad \\qquad &\\epsilon_t \\sim N(0,1) \\\\\n",
    "\\qquad & h_{t} = \\mu + \\phi (h_{t-1} - \\mu) + \\sigma_{\\eta} \\eta_{t}  \\qquad \\qquad &\\eta_{t} \\sim N(0,1) \\\\\n",
    "\\qquad &  h_1 \\sim N(\\mu, \\sigma_\\eta^2/(1-\\phi^2)) \\\\\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The extended stochastic volatility model follows the below setting. The key difference is that $\\sigma_\\eta$ is no more time-invariant, instead, is a random walk with drift decided by covid-19 data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{align*}\n",
    "\\qquad & y_t = \\beta e^{h_t/2} \\epsilon_t\\qquad \\qquad &\\epsilon_t \\sim N(0,1) \\\\\n",
    "\\qquad & h_{t} = \\mu + \\phi (h_{t-1} - \\mu) + \\sigma_{\\eta,t} \\eta_{t}  \\qquad \\qquad &\\eta_{t-1} \\sim N(0,1) \\\\\n",
    "\\qquad &  h_1 \\sim N(\\mu, \\sigma_\\eta^2/(1-\\phi^2)) \\\\\n",
    "\\qquad & \\color{red}{ \\sigma_{\\eta,t} =  \\sigma_{\\eta,t-1} + \\alpha(lnC_t - lnC_{t-1}) + u_t} \\qquad \\qquad &u_t \\sim N(0,1) \\\\\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We plan to sample and calibrate the extended stochastic volatility model. Furthermore, we will conduct a model comparison to check whether the comprehensive model supersedes the canonical model. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import yfinance as yf\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data of VIX Index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we have mentioned before in our project proposal, according to the assumption of the cannonical stochastic volatility model, the distribution of the volatility parameter $\\sigma_\\eta$ does not change with time. We will see that whether the volitility of financial market is time-invariant during covid-19."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we will show the data of CBOE Volatility Index(VIX) during covid-19.The data was downloaded from Yahoo Finance:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load sp500 and vix data use yfinance\n",
    "_finance_data = yf.download(\"^GSPC ^VIX\", start=\"2017-01-01\", end=\"2021-01-11\")['Adj Close']\n",
    "_finance_data = _finance_data.rename({'^GSPC':'SP500', '^VIX':'VIX', '^INDIAVIX':'India_VIX'}, axis=1)\n",
    "_finance_data.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We plot the graph of VIX index:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_finance_data.plot(y=['VIX'],use_index=True, figsize=(15,5),title=\"VIX Index\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our project, we will set 2020-01-22 as the beginning of covid-19.The graph shows that:\n",
    "* Before covid-19,, VIX did not fluctuate greatly.\n",
    "* After covid-19 broke out, espically in the fisrt several months, VIX raised rapidly and reached a peak, which may not be consistent with the assumption that the volatility is time-invariant."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, our group researched on the volatility index of India, Hong Kong,Japan and Eurozone(Actually, it may be a great pity that there is no VIX index in the mainland of China, so we replace it with Hong Kong as it is one of the most important financial centers in the world and its market is closedly related to the mainland's).We downloaded those datas from Google and here are the datas:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDVIX = pd.read_csv(os.path.join(os.curdir,\"Data/IDIndex.csv\")).pipe(pd.DataFrame.rename, columns=lambda x: x.strip()) \\\n",
    ".pipe(pd.DataFrame.rename, {'Close':'IDVIX'}, axis=1) \\\n",
    ".pipe(pd.DataFrame.set_index, ['Date']) \\\n",
    "\n",
    "IDVIX.index = IDVIX.reset_index()['Date'].apply(lambda i : datetime.datetime.strptime(i, '%d-%b-%y'))\n",
    "finance_data = _finance_data.merge(IDVIX, left_index=True, right_index=True)\n",
    "finance_data.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "VHSI = pd.read_csv(os.path.join(os.curdir,\"Data/VHSI.csv\")).pipe(pd.DataFrame.rename, columns=lambda x: x.strip()) \\\n",
    ".pipe(pd.DataFrame.rename, {'Close':'VHSI'}, axis=1) \\\n",
    ".pipe(pd.DataFrame.set_index, ['Date']) \\\n",
    "\n",
    "VHSI.index = VHSI.reset_index()['Date'].apply(lambda i : datetime.datetime.strptime(i, '%d-%b-%y'))\n",
    "finance_data = finance_data.merge(VHSI, left_index=True, right_index=True)\n",
    "finance_data.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "JPVIX = pd.read_csv(os.path.join(os.curdir,\"Data/JPIndex.csv\")).pipe(pd.DataFrame.rename, columns=lambda x: x.strip()) \\\n",
    ".pipe(pd.DataFrame.rename, {'Close':'JPVIX'}, axis=1) \\\n",
    ".pipe(pd.DataFrame.set_index, ['Date']) \\\n",
    "\n",
    "JPVIX.index = JPVIX.reset_index()['Date'].apply(lambda i : datetime.datetime.strptime(i, '%Y.%m.%d'))\n",
    "finance_data = finance_data.merge(JPVIX, left_index=True, right_index=True)\n",
    "finance_data.head(2) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EUVIX = pd.read_csv(os.path.join(os.curdir,\"Data/EUIndex.csv\")).pipe(pd.DataFrame.rename, columns=lambda x: x.strip()) \\\n",
    ".pipe(pd.DataFrame.rename, {'Close':'EUVIX'}, axis=1) \\\n",
    ".pipe(pd.DataFrame.set_index, ['Date']) \\\n",
    "\n",
    "EUVIX.index = EUVIX.reset_index()['Date'].apply(lambda i : datetime.datetime.strptime(i, '%m/%d/%Y'))\n",
    "finance_data = finance_data.merge(EUVIX, left_index=True, right_index=True)\n",
    "finance_data.head(2) "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After collecting the datas, we plot them on the graph:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Finance2=finance_data.drop(columns=['SP500'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(14, 8))\n",
    "fig.delaxes(ax[2,1])\n",
    "ax[0, 0].set_title('VIX',fontsize=20)\n",
    "ax[0, 0].plot(Finance2.index, Finance2.VIX)\n",
    "ax[1, 0].set_title('IDVIX',fontsize=20)\n",
    "ax[1, 0].plot(Finance2.index, Finance2.IDVIX)\n",
    "ax[2, 0].set_title('VHSI',fontsize=20)\n",
    "ax[2, 0].plot(Finance2.index, Finance2.VHSI)\n",
    "ax[0, 1].set_title('JPVIX',fontsize=20,)\n",
    "ax[0, 1].plot(Finance2.index, Finance2.JPVIX)\n",
    "ax[1, 1].set_title('EUVIX',fontsize=20)\n",
    "ax[1, 1].plot(Finance2.index, Finance2.EUVIX)\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "According the the graph, all indices have a great peak after covid-19.(And maybe we can notice that the financial market of Japan is much more stable than that of other financial market)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In conclusion, obviously, the vix indices of these financial market reached a great peak after the broke up of covid-19, which may be a great challenge of \"time-invariant volatility\" consumption. To get more details of it, we will add the data of covid-19 into our research:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data with covid-19"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We download the data of covid-19 from Johns Hopkins Coronavirus Resource Center.By using the methods learned from our classes, we merged these data to a total dataset of all countries and regions we researched on:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_covid_19 =pd.read_csv(os.path.join(os.curdir, \"Data/covid.csv\"))\n",
    "\n",
    "\n",
    "covid_19 = _covid_19.pipe(pd.DataFrame.drop, ['Province/State', 'Lat', 'Long'], 1) \\\n",
    "        .pipe(pd.DataFrame.set_index, 'Country/Region').T\n",
    "        \n",
    "covid_19['global'] = covid_19.apply('sum', axis=1)\n",
    "\n",
    "# calculate the cases growth in each country\n",
    "covid_19_country = pd.DataFrame(index=covid_19.index)\n",
    "for country in covid_19.columns.unique():\n",
    "        covid_19_country['cases_growth_' + country] = np.log(covid_19[[country]].sum(axis=1).replace(0,1)).diff().rolling(14, win_type='gaussian').mean(std=3)                                  \n",
    "\n",
    "\n",
    "# # calculate the cases growth in US\n",
    "# covid_19['cases_growth_US'] = np.log(covid_19['US']).diff().rolling(14, win_type='gaussian').mean(std=3)\n",
    "\n",
    "\n",
    "# calculate the cases growth globally\n",
    "covid_19_country['cases_growth_global'] = covid_19_country.mean(axis=1)\n",
    "\n",
    "covid_19_data = covid_19[['US', 'global']].merge(covid_19_country[['cases_growth_US', \n",
    "                                                                   'cases_growth_global', \n",
    "                                                                   'cases_growth_India',\n",
    "                                                                   'cases_growth_Japan']], \n",
    "                                                 left_index=True, right_index=True)\n",
    "covid_19_data=covid_19_data.drop(columns=['US','global'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_covid_19HK =pd.read_csv(os.path.join(os.curdir, \"Data/HKcovid.csv\"))\n",
    "covid_19HK = _covid_19HK.pipe(pd.DataFrame.drop, ['Province/State', 'Lat', 'Long'], 1) \\\n",
    "        .pipe(pd.DataFrame.set_index, 'Country/Region').T\n",
    "covid_19_HK= np.array(np.log(covid_19HK[['Hong Kong']].sum(axis=1).replace(0,1)).diff().rolling(14, win_type='gaussian').mean(std=3))       \n",
    "covid_19HK['cases_growth_HK']=covid_19_HK"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "covid_19_data=covid_19_data.merge(covid_19HK,left_index=True, right_index=True)\n",
    "covid_19_data=covid_19_data.drop(columns=['Hong Kong'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_covid_19EU =pd.read_csv(os.path.join(os.curdir, \"Data/EUcovid.csv\"))\n",
    "\n",
    "\n",
    "covid_19EU = _covid_19EU.pipe(pd.DataFrame.drop, ['Province/State', 'Lat', 'Long'], 1) \\\n",
    "        .pipe(pd.DataFrame.set_index, 'Country/Region').T\n",
    "        \n",
    "covid_19EU['EU'] = covid_19EU.apply('sum', axis=1)\n",
    "\n",
    "# calculate the cases growth in each country in Eurozone\n",
    "covid_19_EUcountry = pd.DataFrame(index=covid_19EU.index)\n",
    "for country in covid_19EU.columns.unique():\n",
    "        covid_19_EUcountry['cases_growth_' + country] = np.log(covid_19EU[[country]].sum(axis=1).replace(0,1)).diff().rolling(14, win_type='gaussian').mean(std=3)                                  \n",
    "\n",
    "\n",
    "covid_19_EUcountry['cases_growth_EU'] = covid_19_EUcountry.mean(axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "covid_19_data=covid_19_data.merge(covid_19_EUcountry['cases_growth_EU'],left_index=True, right_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "covid_19_data=covid_19_data.drop(index=['Unnamed: 0'])\n",
    "covid_19_data=covid_19_data.fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "covid_19_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After doing these, we start to combine them with the VIX data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Finance_data=finance_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Covid_19_data=covid_19_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Finance_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Covid_19_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = Finance_data.merge(Covid_19_data, left_index=True, right_index=True)\n",
    "data.tail(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firsrly, we show the covid datas of all countries and regions we researched on:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data2=data[['cases_growth_global','cases_growth_US','cases_growth_India','cases_growth_Japan','cases_growth_HK','cases_growth_EU']]\n",
    "data2.index.name='Date'\n",
    "data2.columns.name='Cases_growth'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(3,2,figsize=(20,8))\n",
    "\n",
    "data2.plot(ax=axs,subplots=True,legend=False)\n",
    "axs[0][0].set_title('Cases_growth_global')\n",
    "axs[0][0].set_xlabel('')\n",
    "axs[0][0].tick_params(labelbottom=False)\n",
    "axs[0][1].set_title('Cases_growth_US')\n",
    "axs[0][1].set_xlabel('')\n",
    "axs[0][1].tick_params(labelbottom=False)\n",
    "axs[1][0].set_title('Cases_growth_India')\n",
    "axs[1][0].set_xlabel('')\n",
    "axs[1][0].tick_params(labelbottom=False)\n",
    "axs[1][1].set_title('Cases_growth_Japan')\n",
    "axs[1][1].set_xlabel('')\n",
    "axs[1][1].tick_params(labelbottom=False)\n",
    "axs[2][0].set_title('Cases_growth_HK')\n",
    "axs[2][1].set_title('Cases_growth_EU')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we will draw the covid data and the volatility data of each country or region(the later one will be drawn **with black dotted line** separately. And in each graph we will also draw the graph of the global covid data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the situation of VIX in the United States:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "# make a plot\n",
    "ax.plot(data.index, data.VIX, 'k--',linewidth=3.5)\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"Time\", fontsize=14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"VIX\", fontsize=14)\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(data.index, data['cases_growth_US'], label=\"US\")\n",
    "ax2.plot(data.index, data['cases_growth_global'], label='Global')\n",
    "ax2.set_ylabel(\"log-difference of the confirmed cases\", fontsize=14)\n",
    "ax2.set_title(\"VIX\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the situation of IDVIX in India:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "# make a plot\n",
    "ax.plot(data.index, data.IDVIX, 'k--',linewidth=3.5)\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"Time\", fontsize=14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"IDVIX\", fontsize=14)\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(data.index, data['cases_growth_India'], label=\"India\")\n",
    "ax2.plot(data.index, data['cases_growth_global'], label='Global')\n",
    "#ax2.plot(data.index, data['cases_growth_India'], label='India')\n",
    "ax2.set_ylabel(\"log-difference of the confirmed cases\", fontsize=14)\n",
    "ax2.set_title(\"IDVIX\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the situation of JPVIX in Japan:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "# make a plot\n",
    "ax.plot(data.index,data.JPVIX , 'k--',linewidth=3.5)\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"Time\", fontsize=14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"JPVIX\", fontsize=14)\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(data.index, data['cases_growth_Japan'], label=\"Japan\")\n",
    "ax2.plot(data.index, data['cases_growth_global'], label='Global')\n",
    "#ax2.plot(data.index, data['cases_growth_Japan'], label='Japan')\n",
    "ax2.set_ylabel(\"log-difference of the confirmed cases\", fontsize=14)\n",
    "ax2.set_title('JPVIX')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the situation of VHSI in Hong Kong:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "# make a plot\n",
    "ax.plot(data.index,data.VHSI , 'k--',linewidth=3.5)\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"Time\", fontsize=14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"VHSI\", fontsize=14)\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(data.index, data['cases_growth_HK'], label=\"Hong Kong\")\n",
    "ax2.plot(data.index, data['cases_growth_global'], label='Global')\n",
    "#ax2.plot(data.index, data['cases_growth_HK'], label='Hong Kong')\n",
    "ax2.set_ylabel(\"log-difference of the confirmed cases\", fontsize=14)\n",
    "ax2.set_title('VHSI')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the situation of EUVIX in Eurozone:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "# make a plot\n",
    "ax.plot(data.index,data.EUVIX , 'k--',linewidth=3.5)\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"Time\", fontsize=14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"EUVIX\", fontsize=14)\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(data.index, data['cases_growth_EU'], label=\"EU\")\n",
    "ax2.plot(data.index, data['cases_growth_global'], label='Global')\n",
    "#ax2.plot(data.index, data['cases_growth_EU'], label='EU')\n",
    "ax2.set_ylabel(\"log-difference of the confirmed cases\", fontsize=14)\n",
    "ax2.set_title(\"EUVIX\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From all things above, we can learn something interesting:\n",
    "* The growth of confirmed cases of all these countries/regions rise to a peak between March and April.Although Japan did not has a \"peak period\", its growth on confirmed rates between March and April is also greater than that of other period.\n",
    "* With the growth of confirmed cases, these countries/regions volitility also rise to a peak almost in a simultaneous way.Japan may be a little different from that of others(Its volitility is much lower than others as we have shown),but the time between its \"covid peak\" and \"VIX peak\" is also very closed.\n",
    "* All these volitilites rise to the peak almost at the same time as that of Global confirmed rate. Maybe it demonstrate that the financial markets in the world are connected closedly and the volitility may greatly depend on the global situation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We plot the returns of SP500 index during COVID-19."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Frequentist Model "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fitting SP500 returns by ARCH model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The autoregressive conditional heteroscedasticity (ARCH) model is a statistical model for time series data that describes the variance of the current error term or innovation as a function of the actual sizes of the previous time periods' error terms; often the variance is related to the squares of the previous innovations. The ARCH model is appropriate when the error variance in a time series follows an autoregressive (AR) model; if an autoregressive moving average (ARMA) model is assumed for the error variance, the model is a generalized autoregressive conditional heteroskedasticity (GARCH) model.\n",
    "\n",
    "ARCH models are commonly employed in modeling financial time series that exhibit time-varying volatility and volatility clustering, i.e. periods of swings interspersed with periods of relative calm.\n",
    "\n",
    "To implement ARCH model, we need the following 5 steps:\n",
    "- Check the stationarity of the series \n",
    "- If yes, build an AR model for the mean\n",
    "- Test whether the residual has conditional heteroscedasticity effect\n",
    "- If yes, build an ARCH model for the volatility\n",
    "- Integrate two models together\n",
    "\n",
    "Now we follow the steps and see how the model behave in fitting SP500 returns."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import arch\n",
    "\n",
    "# define load_data function\n",
    "def load_data():\n",
    "    '''\n",
    "    prepare data for the project\n",
    "    :param reload: reload data online\n",
    "    :return: a dataframe\n",
    "    '''\n",
    "\n",
    "    # load sp500 and vix data use yfinance\n",
    "    _finance_data = yf.download(\"^GSPC ^VIX\", start=\"2017-01-01\", end=\"2021-01-11\")['Adj Close']\n",
    "    _finance_data = _finance_data.rename({'^GSPC':'SP500', '^VIX':'VIX', '^INDIAVIX':'India_VIX'}, axis=1)\n",
    "    _finance_data['log_returns'] = np.log(_finance_data['SP500']).diff()\n",
    "    _finance_data['returns'] = _finance_data['SP500'].pct_change()\n",
    "    _finance_data['real_3w_vol'] = _finance_data['returns'].rolling(window=15).apply(pd.DataFrame.std)\n",
    "\n",
    "    # We load covid-19 data from Johns Hopkins Coronavirus Resource Center.\n",
    "    # The raw data is taken a gaussian smoothing with a 2-week window size and standard deviation 3.\n",
    "\n",
    "    _covid_19 = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\",\n",
    "            error_bad_lines=False)\n",
    "\n",
    "\n",
    "    covid_19 = _covid_19.pipe(pd.DataFrame.drop, ['Province/State', 'Lat', 'Long'], 1).pipe(pd.DataFrame.set_index, 'Country/Region').T\n",
    "\n",
    "    covid_19['global'] = covid_19.apply('sum', axis=1)\n",
    "\n",
    "    # calculate the cases growth in each country\n",
    "    covid_19_country = pd.DataFrame(index=covid_19.index)\n",
    "    for country in covid_19.columns.unique():\n",
    "            covid_19_country['cases_growth_' + country] = np.log(covid_19[[country]].sum(axis=1).replace(0,1)).diff().rolling(14, win_type='gaussian').mean(std=3)\n",
    "\n",
    "    # calculate the cases growth globally\n",
    "    covid_19_country['cases_growth_global'] = covid_19_country.mean(axis=1)\n",
    "\n",
    "    covid_19_data = covid_19[['US', 'global']].merge(covid_19_country[['cases_growth_US',\n",
    "                                                                       'cases_growth_global',\n",
    "                                                                       'cases_growth_India']],\n",
    "                                                     left_index=True, right_index=True)\n",
    "    \n",
    "    # set covid state for US, use 0.05 as threshold line\n",
    "    covid_19_data.loc[:, 'covid_state_US'] = 0\n",
    "    covid_19_data.loc[covid_19_data.cases_growth_US > 0.05, 'covid_state_US'] = 1\n",
    "\n",
    "    covid_19_data.index = covid_19_data.reset_index()['index'].apply(lambda i : datetime.datetime.strptime(i, '%m/%d/%y'))\n",
    "\n",
    "    # merge data\n",
    "    data = finance_data.merge(covid_19_data, left_index=True, right_index=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "data = load_data()\n",
    "data['returns'] =  np.log(data.SP500 / data.SP500.shift(1)).fillna(0) * 100\n",
    "data.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "####  Step 1 - Check the stationarity of SP500 returns\n",
    "\n",
    "First we use ADF test to check whether the returns of SP500 is stationary. That is:\n",
    "\n",
    "- Null hypothesis $H_0$: the sequence is non-stationary \n",
    "\n",
    "- Alternative hypothesis $H_1$: the sequence is stationary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ADF test\n",
    "t = sm.tsa.stattools.adfuller(data.returns) \n",
    "print (\"p-value: \",t[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The p-value is less than the significance level, so the null hypothesis is rejected, Therefore. the sequence is stationary. \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 2 - Build an AR model\n",
    "Autoregressive(AR) model is a statistical method of processing time series, using the same variable such as the previous periods of $x$, that is, $x_1$ to $x_{t-1}$ to predict the performance of $x_t$ in this period. And suppose they are a linear relationship. In this content, the AR model can be expressed as:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\qquad & r_t = c_1 + \\sum_{i=1}^L{\\phi_i r_{t-i}} + \\epsilon_t \\qquad &\\epsilon_t \\sim N(0,\\sigma^2) \\\\\n",
    "\\end{align*}$$<br>\n",
    "\n",
    "We use the partial autocorrelation function PACF to determine L."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot partial autocorrelation for AR model\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "fig = sm.graphics.tsa.plot_pacf(data.returns,lags = 20,ax = ax1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the graph, we choose $L=3$ and build an AR(3) model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set L=3\n",
    "L = (3,0)\n",
    "# build AR model\n",
    "ar_model = sm.tsa.ARMA(list(data.returns), L).fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 3 - Test the autocorrelation of the residual\n",
    "\n",
    "Let $\\epsilon_t$ be the residuals\n",
    "\n",
    "$$\\epsilon_t = r_t - \\hat{r}_t$$\n",
    "\n",
    "We use Ljung-Box test to test the correlation of the sequence $\\{\\epsilon_t^2\\}$  to determine whether it has the ARCH effect: \n",
    "\n",
    "- Null hypothesis $H_0$: the sequence has no serial correlation\n",
    "\n",
    "- Alternative hypothesis $H_1$: the sequence has serial correlation\n",
    "\n",
    "If the answer is yes, we will need to use another model to adapt to this situation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the residuals\n",
    "eps_t = data.returns -  ar_model.fittedvalues\n",
    "eps_t2 = np.square(eps_t)\n",
    "\n",
    "# Plot the residuals and the square of the residuals\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,1,1)\n",
    "plt.plot(eps_t2,label='ϵ_t square')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test first 10 correlation coefficients\n",
    "test_n = 10\n",
    "# calculate correlation coefficients and p-value\n",
    "acf, q, p = sm.tsa.acf(eps_t2, nlags = test_n, qstat = True, fft = False) \n",
    "out = np.c_[range(1,1+test_n), acf[1:], q, p]\n",
    "test_output = pd.DataFrame(out, columns=['lag', \"AC\", \"Q\", \"P-value\"]).set_index('lag')\n",
    "\n",
    "print(\"Ljung-Box test: \\n\")\n",
    "print(test_output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the above result, all 10 p-values are less than the significance level of 0.05, so we reject the null hypothesis that the sequence has no autocorrelation. Therefore, it has the ARCH effect."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 4 - Build an ARCH model for the volatility\n",
    "\n",
    "By the previous step, we conclude that AR model alone is not good enough to model the SP500 returns. We need to use ARCH model to capture the conditional heteroskedasticity of the residuals. The ARCH model can be expressed as:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\qquad & \\epsilon_t = u_t \\sigma_t  \\qquad &u_t \\sim N(0,1) \\\\\n",
    "\\qquad & \\sigma_t^2 = k + \\sum_{i=1}^p{\\alpha_i \\sigma_{t-i}^2} \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "To determine the order p of the ARCH model, we again use the partial autocorrelation function PACF of $\\{\\epsilon^2_t\\}$ sequence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot partial autocorrelation for ARCH model\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "fig = sm.graphics.tsa.plot_pacf(eps_t2,lags = 30,ax = ax1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the figure above, we can let $p = 8$ and choose ARCH(8) as volatility model. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 5 - Integrate the above two models into an AR-ARCH model\n",
    "Based on our previous analysis, we can roughly choose the mean model as the AR(2) model, and the volatility model as the ARCH(8) model.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\qquad & r_t = c_1 + \\sum_{i=1}^3{\\phi_i r_{t-i}} + \\epsilon_t \\\\\n",
    "\\qquad & \\epsilon_t = u_t \\sigma_t  \\qquad &u_t \\sim N(0,1) \\\\\n",
    "\\qquad & \\sigma_t^2 = k + \\sum_{i=1}^{8}{\\alpha_i \\sigma_{t-i}^2} \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "We use the arch package in Python to implement the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build AR-ARCH model\n",
    "ar_arch = arch.arch_model(data.returns,mean='AR',lags=3,vol='ARCH',p=8)\n",
    "# fit the SP500 returns data\n",
    "res_ar_arch = ar_arch.fit()\n",
    "# show results\n",
    "res_ar_arch.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We get our fitting model:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\qquad & r_t = 0.1324 - 0.0778 r_{t-1} + 0.1682 r_{t-2} + \\epsilon_t \\\\\n",
    "\\qquad & \\epsilon_t = u_t \\sigma_t  \\qquad & u_t \\sim N(0,1) \\\\\n",
    "\\qquad & \\sigma_t^2 = 0.7844 + 0.1714 \\sigma_{t-1}^2 + 0.4520 \\sigma_{t-2}^2 + 0.1275 \\sigma_{t-6}^2 \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "From the statistic, our model is not so suitable, since the R square and adjusted R square are both low, and AIC, BIC are both high. We then take a loot at the fitting result to find our problems."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = res_ar_arch.hedgehog_plot(plot_type='volatility')\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the picture, the blue line is the original return rate volatility sequence, and the orange line is the predicted volatility sequence. It can be found that the spikes during 2020 March to 2020 May causes the model to overestimate the averge volatility. As a result, despite volatility went back to normal in the latter period, our model still predicts that the volatility would go up to a higher level. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mini-conclusion\n",
    "\n",
    "AR-ARCH model are commonly employed in modeling financial time series that exhibit time-varying volatility and volatility clustering. But in COVID era, it performs poorly due to extreme volatility caused by outer shocks. As a result, we must incorporate the compact of COVID into our model to better fitting the volatility spikes. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fitting US VIX by ARX model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to add COVID as an exogenous variable in the GARCH model. Unfortunately, Python has no packages that can solve GARCH-X model, so we simplify the situation and switch to ARX model that use COVID as an exogenous variable to fit US VIX index.\n",
    "\n",
    "First we plot the graph for VIX index and log of the COVID cases growth in US."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get VIX data\n",
    "sp500_vol = data.VIX\n",
    "# drop the irrelavant data\n",
    "sp500_vol = sp500_vol.loc['2020-02-05':].fillna(method='ffill')\n",
    "# get COVID data\n",
    "covid_us = data.cases_growth_US * 100\n",
    "# drop the irrelavant data\n",
    "covid_us = covid_us.loc['2020-02-05':]\n",
    "# plot the figure\n",
    "sp500_vol.plot(figsize=(15,5))\n",
    "covid_us.plot()\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we use ARX to fit the data. That is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\qquad & vol_t = c_1 + \\phi_1 vol_{t-1} + \\alpha_1 covid_t + \\epsilon_t \\qquad &\\epsilon_t \\sim N(0,\\sigma^2)\\\\\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from arch.univariate import ARX, Normal\n",
    "# build ARX model\n",
    "arx_model = ARX(y = sp500_vol, x = covid_us, lags=1, distribution=Normal())\n",
    "# fit our data\n",
    "res_arx = arx_model.fit()\n",
    "# show results\n",
    "res_arx.summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our fitting model is:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\qquad & vol_t = 5.5934 + 0.7749 vol_{t-1} + 0.2969 covid_t + \\epsilon_t \\qquad &\\epsilon_t \\sim N(0,13.6023)\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "It is good news that all the p-values are less than the significant level and adjusted R square is large. However, we still see that AIC and BIC is high, which is not satisfying. Also, we need to check whether the model has the ARCH effect."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the residuals and original data\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(15, 5),)\n",
    "ax2 = ax1.twinx()\n",
    "blue = tuple(np.array([78, 145, 224]) / 255)\n",
    "red = tuple(np.array([205, 0, 0]) / 255)\n",
    "ax1.plot(sp500_vol,label='origin_data',color=red)\n",
    "ax2.plot(res_arx.std_resid,label='standardized_residuals',color=blue)\n",
    "ax1.set_yticks(np.arange(-80,120,40))\n",
    "ax2.set_yticks(np.arange(-4,6,2))\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "legend1 = fig.legend(handles1+handles2, labels1+labels2,loc='lower center',ncol=2,frameon=False)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By the results above, the model has the problem of conditional heteroskedasticity, so ARX with COVID is not enough to model the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mini-conclusion\n",
    "\n",
    "while adding COVID data into our frequentist model significantly improves our result, it still has several issues. First, there is no ARCH-X or GARCH-X model that we can use in current Python environment, so the result above is just an approximation by VIX data, which does not accurately demonstrate the volatility of real SP500 volatility. Second, our ARX model has ARCH effect, which means ARX alone can not fully capture the patterns of VIX index."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### From Random Walk to Free Scale - The Evolution of Stochastic Volatility Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baseline Model: RW Stochastic Volatility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start from the most basic stochastic volatility model, namely,\n",
    "volatility follows a random walk latent process without any exogenous\n",
    "variables.\n",
    "\n",
    "Mathamatically speaking,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\begin{align*}\n",
    "\\qquad & y_t = \\beta e^{h_t/2} \\epsilon_t\\qquad \\qquad &\\epsilon_t \\sim N(0,1) \\\\\n",
    "\\qquad & h_{t} = \\mu + \\phi (h_{t-1} - \\mu) + \\sigma_{\\eta} \\eta_{t}  \\qquad \\qquad &\\eta_{t} \\sim N(0,1) \\\\\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use an informative prior, setting $\\beta = 1$ and $\\mu = 0$ to facilitate\n",
    "computation and subsequent analysis.\n",
    "##### Model Setting\n",
    "The likelihood is given by\n",
    "\\begin{align*}\n",
    "\\qquad & log(y_t) \\sim StudentT(\\nu,0, e^{h_t}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The priors are given by\n",
    "\\begin{align*}\n",
    "\\qquad & \\nu \\sim Exp(0.1) \\\\\n",
    "\\qquad & \\sigma_\\eta \\sim Exp(10) \\\\\n",
    "\\qquad & h_{t} = h_{t-1} + \\sigma_{\\eta}* \\eta_{t}   \\\\\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use NUTS to sample the Random Walk Model.\n",
    "\n",
    "Also we define some common-used utils functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Utils Functions\n",
    "\n",
    "def load_data(regular = True, reload=False):\n",
    "    '''\n",
    "    prepare data for the project\n",
    "    :param regular: for developers only\n",
    "    :param reload: reload data online\n",
    "    :return: a dataframe\n",
    "    '''\n",
    "    if not reload:\n",
    "        if os.path.exists(\"../Data/data.pkl\"):\n",
    "            data = pd.read_pickle(\"../Data/data.pkl\")\n",
    "            return data\n",
    "        else:\n",
    "            return load_data(reload=True)\n",
    "\n",
    "    # load sp500 and vix data use yfinance\n",
    "    _finance_data = yf.download(\"^GSPC ^VIX\", start=\"2017-01-01\", end=\"2021-01-11\")['Adj Close']\n",
    "    _finance_data = _finance_data.rename({'^GSPC':'SP500', '^VIX':'VIX', '^INDIAVIX':'India_VIX'}, axis=1)\n",
    "    _finance_data['log_returns'] = np.log(_finance_data['SP500']).diff()\n",
    "    _finance_data['returns'] = _finance_data['SP500'].pct_change()\n",
    "    _finance_data['real_3w_vol'] = _finance_data['returns'].rolling(window=15).apply(pd.DataFrame.std)\n",
    "\n",
    "    try:\n",
    "        VIX_india = pd.read_csv(\"indian vix.csv\").pipe(pd.DataFrame.rename, columns=lambda x: x.strip()) .pipe(\n",
    "            pd.DataFrame.rename, {'Close':'India_VIX'}, axis=1) .pipe(pd.DataFrame.set_index, ['Date'])\n",
    "    except FileNotFoundError:\n",
    "        VIX_india = pd.read_csv(\"../indian vix.csv\").pipe(pd.DataFrame.rename, columns=lambda x: x.strip()).pipe(\n",
    "            pd.DataFrame.rename, {'Close': 'India_VIX'}, axis=1).pipe(pd.DataFrame.set_index, ['Date'])\n",
    "    VIX_india.index = VIX_india.reset_index()['Date'].apply(lambda i : datetime.datetime.strptime(i, '%d-%b-%y'))\n",
    "    finance_data = _finance_data.merge(VIX_india, left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    # We load covid-19 data from Johns Hopkins Coronavirus Resource Center.\n",
    "    # The raw data is taken a gaussian smoothing with a 2-week window size and standard deviation 3.\n",
    "\n",
    "\n",
    "    # load covid-19 data from Johns Hopkins Coronavirus Resource Center\n",
    "    # code for Bo Sun only\n",
    "    if regular:\n",
    "        _covid_19 = pd.read_csv(\"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\",\n",
    "                error_bad_lines=False)\n",
    "    else:\n",
    "        _covid_19 = pd.read_csv(r\"C:\\Users\\harvey_sun\\Desktop\\data\\time_series_covid19_confirmed_global.csv\")\n",
    "\n",
    "\n",
    "    covid_19 = _covid_19.pipe(pd.DataFrame.drop, ['Province/State', 'Lat', 'Long'], 1)         .pipe(pd.DataFrame.set_index, 'Country/Region').T\n",
    "\n",
    "    covid_19['global'] = covid_19.apply('sum', axis=1)\n",
    "\n",
    "    # calculate the cases growth in each country\n",
    "    covid_19_country = pd.DataFrame(index=covid_19.index)\n",
    "    for country in covid_19.columns.unique():\n",
    "            covid_19_country['cases_growth_' + country] = np.log(covid_19[[country]].sum(axis=1).replace(0,1)).diff().rolling(14, win_type='gaussian').mean(std=3)\n",
    "\n",
    "    # calculate the cases growth globally\n",
    "    covid_19_country['cases_growth_global'] = covid_19_country.mean(axis=1)\n",
    "\n",
    "    covid_19_data = covid_19[['US', 'global']].merge(covid_19_country[['cases_growth_US',\n",
    "                                                                       'cases_growth_global',\n",
    "                                                                       'cases_growth_India']],\n",
    "                                                     left_index=True, right_index=True)\n",
    "    # set covid state for US, use 0.05 as threshold line\n",
    "    covid_19_data.loc[:, 'covid_state_US'] = 0\n",
    "    covid_19_data.loc[covid_19_data.cases_growth_US > 0.05, 'covid_state_US'] = 1\n",
    "\n",
    "    covid_19_data.index = covid_19_data.reset_index()['index'].apply(lambda i : datetime.datetime.strptime(i, '%m/%d/%y'))\n",
    "\n",
    "    # merge data\n",
    "    data = finance_data.merge(covid_19_data, left_index=True, right_index=True)\n",
    "    if not os.path.exists(\"../Data\"):\n",
    "        os.mkdir(\"../Data\")\n",
    "    data.to_pickle(\"../Data/data.pkl\")\n",
    "    return data\n",
    "\n",
    "def model_diagnose(model, trace, var_names):\n",
    "    '''\n",
    "    diagnose a model based on 'Effective Sample Size and Rhat\n",
    "\n",
    "    :param model: a PyMC3 model\n",
    "    :param trace: sample trace\n",
    "    :param var_names: variable names\n",
    "    :return: None\n",
    "    '''\n",
    "    ess = az.ess(trace, relative=True)\n",
    "\n",
    "    print(\"Effective Sample Size (min across parameters)\")\n",
    "    for var in var_names:\n",
    "        print(f\"\\t{var}: {ess[var].values.min()}\")\n",
    "    rhat = az.rhat(trace)\n",
    "\n",
    "    print(\"rhat (max across parameters)\")\n",
    "    for var in var_names:\n",
    "        print(f\"\\t{var}: {rhat[var].values.max()}\")\n",
    "\n",
    "def gen_xy(trace, _data, y=\"log_vol\", AR=False, skip=5):\n",
    "    '''\n",
    "    generate posterior predictive y from trace\n",
    "\n",
    "    :param trace: sample trace\n",
    "    :param _data: observation data\n",
    "    :param y: y column anme\n",
    "    :param AR: whether it's a AR model\n",
    "    :param skip: how often take draws from trace, only take 1 draw after skip steps\n",
    "    :return: x values from observation data, y values from posterior predictive\n",
    "    '''\n",
    "    _y_vals = np.exp(trace.posterior[y])\n",
    "    y_vals = np.vstack([_y_vals[i] for i in range(_y_vals.shape[0])]).T\n",
    "    if AR:  # take the last n-1 values, because AR1 process has an extra starting point\n",
    "        y_vals = y_vals[:-1, ]\n",
    "    # only take 1 draw after skip steps\n",
    "    y_vals = y_vals.T[::skip].T\n",
    "    x_vals = np.vstack([_data.index for _ in y_vals.T]).T.astype(np.datetime64)\n",
    "    return x_vals, y_vals\n",
    "\n",
    "def model_plot(data, trace, pp, AR=False):\n",
    "    '''\n",
    "    plot returns and volatility\n",
    "    :param data: observation data\n",
    "    :param trace: sample trace\n",
    "    :param pp: posterior predictive\n",
    "    :param AR: whether it's a AR model\n",
    "    :return: figures\n",
    "    '''\n",
    "    x = data.index.to_numpy().astype(np.datetime64)\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "    # Plot returns\n",
    "    ax[0].plot(\n",
    "        x, pp[\"returns\"][::10].T, color=\"g\",\n",
    "        alpha=0.25, zorder=-10\n",
    "    )\n",
    "    ax[0].plot(x, data[\"log_returns\"].to_numpy(), color=\"k\", linewidth=2.5, label=\"log returns\")\n",
    "    ax[0].set(title=\"Posterior predictive log-returns (green) VS actual log-returns\", ylabel=\"Log Returns\")\n",
    "\n",
    "    # Plot volatility\n",
    "    _, y_vals = gen_xy(trace, data, AR=AR, skip=10)\n",
    "    ax[1].plot(x, y_vals, \"k\", alpha=0.01)\n",
    "    ax[1].plot(data.index, data['real_3w_vol'], linewidth=2.5, label=\"realized 3W vol\")\n",
    "    ax[1].set(title=\"Estimated volatility over time (balck) vs realized vol\", ylabel=\"Volatility\")\n",
    "    ax[1].set_ylim(bottom=0)\n",
    "\n",
    "    # Add legends\n",
    "    ax[0].legend(loc=\"upper right\")\n",
    "    ax[1].legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "data = load_data(reload=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code for Random Walk\n",
    "def make_baseline_model_RW(data, observe):\n",
    "    '''model for Random Walk StoVol'''\n",
    "    with pm.Model() as model:\n",
    "        # Piror\n",
    "        nu = pm.Exponential(\"nu\", 0.1)\n",
    "        scale = pm.Exponential(\"scale\", 10)\n",
    "        log_vol = pm.GaussianRandomWalk(\"log_vol\", sigma=scale, shape=len(data))\n",
    "        # Likelihood\n",
    "        returns = pm.StudentT(\"returns\", nu=nu, lam=np.exp(-2 * log_vol), observed=data[observe])\n",
    "    return model\n",
    "\n",
    "baseline_model_RW = make_baseline_model_RW(data, \"log_returns\")\n",
    "\n",
    "with baseline_model_RW:\n",
    "    trace_RW = pm.sample(2000, tune=2000, return_inferencedata=True)\n",
    "    _var_names = ['nu', 'scale']\n",
    "    model_diagnose(baseline_model_RW, trace_RW, _var_names)\n",
    "    az.plot_trace(trace_RW, var_names=_var_names)\n",
    "    pp_RW = pm.sample_posterior_predictive(trace_RW)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We plot the posterior predictive returns and volatility.\n",
    "\n",
    "There are 4 chains and each chian have 2000 steps.\n",
    "\n",
    "We take every 5th of the draws and yield 1600 paths."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_plot(data=data, trace=trace_RW, pp=pp_RW)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Extension: AR1 Stochastic Volatility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall the stochastic volatility model is defined by,\n",
    "\n",
    "\\begin{align*}\n",
    "\\qquad & y_t = e^{h_t/2} \\epsilon_t\\qquad \\qquad &\\epsilon_t \\sim N(0,1) \\\\\n",
    "\\qquad & h_{t} - \\mu= \\phi(h_{t-1} - \\mu) + \\sigma_{\\eta} \\eta_{t}  \\qquad \\qquad &\\eta_{t} \\sim N(0,1) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The Random Walk Stochastic Volatility Model coerces the correlation\n",
    "between $h_{t}$ and $h_{t-1}$ to be 1 (in our case, $\\phi=1$).\n",
    "\n",
    "However, it doesn't have to\n",
    "be 1, and hence, we free up that parameter and change the model\n",
    "setting to be a AR1 process.\n",
    "\n",
    "Note that $\\mu$ is not being set to 0 (like what we have done in\n",
    "Random Walk Stochastic Volatiltiy Model), instead, we treat $\\mu$\n",
    "as a model parameter to fit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model Setting\n",
    "The likelihood is given by\n",
    "\\begin{align*}\n",
    "\\qquad & log(y_t) \\sim StudentT(\\nu,\\mu, e^{h_t}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The priors are given by\n",
    "\\begin{align*}\n",
    "\\qquad & \\nu \\sim Exp(0.1) \\\\\n",
    "\\qquad & \\phi \\sim \\mathcal{N}(0, 1) \\\\\n",
    "\\qquad & \\sigma_\\eta \\sim Exp(10) \\\\\n",
    "\\qquad & \\mu \\sim \\mathcal{N}(0, 1) \\\\\n",
    "\\qquad & h_{t} - \\mu = \\phi(h_{t-1} - \\mu)+ \\sigma_\\eta* \\eta_{t}   \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We beef up the tuning iterations from 2k to 4k as the model complexity increases.\n",
    "\n",
    "Also, we hardcode the random seed as 1234 to avoid initialization failure."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code for AR1\n",
    "def make_baseline_model_AR1(data, observe):\n",
    "    '''\n",
    "    model for AR1 StoVol\n",
    "\n",
    "    :param data: observation data\n",
    "    :param observe: column name of y\n",
    "    :return: PyMC model\n",
    "    '''\n",
    "    with pm.Model() as model:\n",
    "        # Piror\n",
    "        # phi = pm.Beta(\"phi\", alpha=20, beta=1.5)\n",
    "        np.random.seed(12345)\n",
    "        phi = pm.Normal(\"phi\", mu=1, sigma=1, testval=np.random.randn())\n",
    "        # scale = pm.InverseGamma(\"scale\", alpha=2.5, beta=0.05)\n",
    "        scale = pm.Exponential(\"scale\", 10, testval=np.random.randn())\n",
    "        _log_vol = pm.AR1(\"_log_vol\", k=phi, tau_e=1/pm.math.sqr(scale), shape=len(data), testval=np.random.randn(len(data)))\n",
    "        # mu = pm.Exponential('mu', lam=0.1)\n",
    "        mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "        log_vol = pm.Deterministic(\"log_vol\", _log_vol + mu)\n",
    "        mean_return = pm.Normal(\"mean_return\", 0, 1)\n",
    "        nu = pm.Exponential(\"nu\", 0.1)\n",
    "        # Likelihood\n",
    "        # returns = pm.Normal(\"returns\", mu=mean_return, sigma=np.exp(log_vol/2), observed=data[observe])\n",
    "        returns = pm.StudentT(\"returns\", nu=nu, mu=mean_return, lam=np.exp(-2 * log_vol), observed=data[observe])\n",
    "    return model\n",
    "\n",
    "baseline_model_AR = make_baseline_model_AR1(data, \"log_returns\")\n",
    "\n",
    "with baseline_model_AR:\n",
    "    trace_AR = pm.sample(2000, tune=4000, return_inferencedata=True)\n",
    "    _var_names = [\"mean_return\", \"phi\", \"scale\", \"mu\", \"nu\"]\n",
    "    model_diagnose(baseline_model_AR, trace_AR, _var_names)\n",
    "    az.plot_trace(trace_AR, var_names=_var_names)\n",
    "    pp_AR = pm.sample_posterior_predictive(trace_AR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<pre>\n",
    "Simplicity is the ultimate sophistication.\n",
    "                                --Leonardo da Vinci\n",
    "</pre>\n",
    "The posterior distributions echo the Leonardo da Vinci famous saying.\n",
    "\n",
    "Even though we have allowed more free parameters in the model, neither do we\n",
    "receive noticeable model improvements as shown in graphs below compared to the\n",
    "more parsimonious random walk model, nor do the posteriors distrubtions provide\n",
    "sufficient evidences.\n",
    "\n",
    "The posteriors of mean_return center around 0.002, $\\phi$ cluster at 1 and $\\mu$\n",
    "is spreading across the support.\n",
    "\n",
    "This results shows that assumptions under classical stochastic\n",
    "volatility model are mostly reasonable and supported by empirical data.\n",
    "To be more specific, mean of log-return is 0, log-volatility is a\n",
    "random walk process and the mean of log-volatility is 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_plot(data=data, trace=trace_AR, pp=pp_AR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Extension: Two-State Stochastic Volatility"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall the stylized fact that COVID-19 greatly impacts realized and implied\n",
    "volatility and market indices per se, which motivate us to incorporate COVID data\n",
    "in volatility analysis.\n",
    "\n",
    "We initiate a two-stage model - high-vol stage and low-vol stage,\n",
    "which is identifed by COVID-19. A period with high COVID\n",
    "infection number is in high-vol state, while a period with low COVID\n",
    "infection is in low-vol state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model Setting\n",
    "The likelihood is given by\n",
    "\\begin{align*}\n",
    "\\qquad & log(y_t) \\sim StudentT(\\nu,\\mu, e^{h_t}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The priors are given by\n",
    "\\begin{align*}\n",
    "\\qquad & \\nu \\sim Exp(0.1) \\\\\n",
    "\\qquad & \\phi \\sim \\mathcal{N}(0, 1) \\\\\n",
    "\\qquad & scale \\sim Exp(10) \\\\\n",
    "\\qquad & h_{t} = \\phi h_{t-1}+ scale_s* \\epsilon_{t} \\qquad s \\in (low\\_vol, high\\_vol)  \\\\\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exploring with the method of trial and errors, we argue that log-difference\n",
    "log-difference of the confirmed cases being 5% is a decent cut-off point to seperate\n",
    "high-vol and low-vol state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,5))\n",
    "# make a plot\n",
    "ax.plot(data.index, data.VIX, color=\"red\")\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"Time\", fontsize=14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"VIX\", fontsize=14)\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(data.index, data['cases_growth_US'], label=\"US\")\n",
    "ax2.axhline(y=0.05, color='black', linestyle='--', linewidth=2.5)\n",
    "ax2.set_ylabel(\"log-difference of the confirmed cases\", fontsize=14)\n",
    "ax2.set_title(\"5% is a decent cutoff point\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code for Two-State Model\n",
    "def make_state_model(data, observe):\n",
    "    '''\n",
    "    model for Two-State StoVol\n",
    "\n",
    "    :param data: observation data\n",
    "    :param observe: column name of y\n",
    "    :return: PyMC model\n",
    "    '''\n",
    "    # Prepare data\n",
    "    nstate = data['covid_state_US'].nunique()\n",
    "    log_returns = data[observe].to_numpy()\n",
    "    state_idx = data[\"covid_state_US\"].to_numpy()\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Data\n",
    "        _returns = pm.Data(\"_returns\", log_returns)\n",
    "        _state_idx = pm.intX(pm.Data(\"state_idx\", state_idx))\n",
    "        # Prior\n",
    "        scale = pm.InverseGamma(\"scale\", alpha=2.5, beta=0.05, shape=nstate)\n",
    "        log_vol = pm.GaussianRandomWalk('log_vol', mu=0, sigma=scale[_state_idx], shape=len(data))\n",
    "        nu = pm.Exponential(\"nu\", 0.1)\n",
    "        # Likelihood\n",
    "        returns = pm.StudentT(\"returns\", nu=nu, lam=np.exp(-2 * log_vol), observed=_returns)\n",
    "    return model\n",
    "\n",
    "model_state = make_state_model(data, \"log_returns\")\n",
    "\n",
    "with model_state:\n",
    "    trace_state = pm.sample(2000, tune=2000, return_inferencedata=True)\n",
    "    _var_names = [\"scale\", \"nu\"]\n",
    "    model_diagnose(model_state, trace_state, _var_names)\n",
    "    az.plot_trace(trace_state, var_names=_var_names)\n",
    "    pp_state = pm.sample_posterior_predictive(trace_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The orange chains of scale paramters stands for $\\sigma_\\eta$ in\n",
    "high-infection time while the blue chairs are in low-infection time.\n",
    "\n",
    "The posterior distribution of the scale pamater $\\sigma_\\eta$ tells\n",
    "a very attractive story -  the latent volatility process is mild and\n",
    "steady under gentle contagious spread, but if covid-19 spreads quickly,\n",
    "the volatility process also go wild.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_plot(data=data, trace=trace_state, pp=pp_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We compare the Two-state model vs the Random walk model.\n",
    "\n",
    "The black paths are estimated volatility from two-state model,\n",
    "and red paths arefrom random walk model.\n",
    "The two-state model paths are enveloped by that of randome walk model,\n",
    "illustrating that two-state model provide a narrower range of\n",
    "posterior prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "\n",
    "x_vals, y_vals = gen_xy(trace_state, data)\n",
    "plt.plot(x_vals, y_vals, \"black\", alpha=0.002)\n",
    "\n",
    "# comparison with baseline model\n",
    "x_vals, y_vals = gen_xy(trace_RW, data)\n",
    "plt.plot(x_vals, y_vals, \"red\", alpha=0.002)\n",
    "plt.plot(data.index, data['real_3w_vol'], linewidth=2.5)\n",
    "ax.set_xlim(x_vals.min(), x_vals.max())\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set(title=\"Estimated volatility over time - \"\n",
    "             \"two-state model (black) provide better prediction than random walk model (red) \",\n",
    "       xlabel=\"Date\", ylabel=\"Volatility\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Extension: Free-Scale Stochastic Volatility\n",
    "\n",
    "The two-state model sheds light on the interrelation betwen market\n",
    "volatility and COVID infection.\n",
    "\n",
    "We are explorers and unsatisfied about the discrete state-space\n",
    "representation of the model.\n",
    "\n",
    "Hence we decide to do some heavylifting - introducing covid-19\n",
    "as exogeneous variable to the volatiltiy process.\n",
    "\n",
    "More specifically, we set the scale\n",
    "parameter $\\sigma_\\eta$ to be a random walk process with a drift\n",
    "determined by COVID infection and here comes the name\n",
    "**Free-Scale Stochastic Volatility** as $\\sigma_\\eta$ is termed\n",
    "*scale* in the literature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Model Setting\n",
    "The likelihood is given by\n",
    "\\begin{align*}\n",
    "\\qquad & log(y_t) \\sim StudentT(\\nu,\\mu, e^{h_t}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The priors are given by\n",
    "\\begin{align*}\n",
    "\\qquad & \\nu \\sim Exp(0.1) \\\\\n",
    "\\qquad & \\phi \\sim \\mathcal{N}(1, 1) \\\\\n",
    "\\qquad & h_{t} = \\phi h_{t-1}+ \\sigma_{\\eta, t}* \\eta_{t} \\\\\n",
    "\\qquad & \\sigma_{\\eta, t} = \\sigma_{\\eta, t-1} + \\alpha*(lnC_t - lnC_{t-1}) + u_t \\\\\n",
    "\\end{align*}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start from the easiest -  assume the log-vol process is a random walk ($\\phi=1$)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code for Free-Scale Stochastic Volatility assumeing a RW log-vol process\n",
    "\n",
    "def make_covid_model(data, observe, col_covid=\"cases_growth_US\", process='GRW'):\n",
    "    '''\n",
    "    model for Free-Scale StoVol\n",
    "\n",
    "    :param data: observation data\n",
    "    :param observe: column name of y\n",
    "    :param col_covid: column name of covid data\n",
    "    :param process: process of the scale paramter, can be GRW or AR1\n",
    "    :return: PyMC model\n",
    "    '''\n",
    "\n",
    "    if data[col_covid].hasnans:\n",
    "        raise ValueError(f\"{col_covid} has NaN values\")\n",
    "\n",
    "    log_returns = data[observe].to_numpy()\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Data\n",
    "        _returns = pm.Data(\"_returns\", log_returns)\n",
    "        # _change_returns = pm.Data(\"_change_returns\", data[observe_str], dims=observe_str, export_index_as_coords=True)\n",
    "        _covid = pm.Data(\"covid\", data[col_covid])\n",
    "\n",
    "        # HyperPrior\n",
    "        alpha = pm.Normal(\"alpha\", mu=1, sigma=1, testval=np.random.random())\n",
    "        scale = pm.GaussianRandomWalk(\"scale\", mu=alpha*_covid, sigma=1, shape=len(data),\n",
    "                                      testval=np.random.randint(low=1, high=10, size=len(data)))\n",
    "        # Prior\n",
    "        if process == 'GRW':  # scale follows a Gaussian Random Walk\n",
    "            log_vol = pm.GaussianRandomWalk(\"log_vol\", sigma=scale, shape=len(data),\n",
    "                                        testval=np.random.randint(low=1, high=10, size=len(data)))\n",
    "        elif process == 'AR1': # scale follows a AR1\n",
    "            phi = pm.Beta(\"phi\", alpha=20, beta=1.5)\n",
    "            # phi = pm.Normal(\"phi\", mu=1, sigma=1, testval=np.random.randint(low=1, high=10))\n",
    "            log_vol = pm.AR1(\"log_vol\", k=phi, tau_e=1 / pm.math.sqr(scale), shape=len(data)+1,\n",
    "                             testval=np.random.randint(low=1, high=10, size=len(data)+1))[:-1]\n",
    "        nu = pm.Exponential(\"nu\", 0.1)\n",
    "\n",
    "        # Likilihood\n",
    "        returns = pm.StudentT(\"returns\", nu=nu, lam=np.exp(-2 * log_vol), observed=_returns)\n",
    "    return model\n",
    "\n",
    "data_cmodel = data.dropna(subset=['cases_growth_US'])\n",
    "model_covid = make_covid_model(data_cmodel, \"log_returns\")\n",
    "\n",
    "with model_covid:\n",
    "    trace_covid = pm.sample(3000, tune=4000, return_inferencedata=True)\n",
    "    _var_names = [\"alpha\", \"nu\"]\n",
    "    model_diagnose(model_covid, trace_covid, _var_names)\n",
    "    az.plot_trace(trace_covid, var_names=_var_names)\n",
    "    pp_covid = pm.sample_posterior_predictive(trace_covid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_plot(data=data_cmodel, trace=trace_covid, pp=pp_covid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Free-Scale Stochastic Volatility behave disappointing - it's even\n",
    "worse than the Two-state model, despite of much more complexity added.\n",
    "\n",
    "We are wondering whether the moderate performance results from the\n",
    "assmumption that log-vol process is a random walk, and hence\n",
    "decide to sample the model with a AR1 log-vol process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code for Free-Scale Stochastic Volatility assumeing a AR log-vol process\n",
    "data_cmodel = data.dropna(subset=['cases_growth_US'])\n",
    "model_covid_AR = make_covid_model(data_cmodel, \"log_returns\", process='AR1')\n",
    "\n",
    "with model_covid_AR:\n",
    "    trace_covid_AR1 = pm.sample(3000, tune=4000, return_inferencedata=True)\n",
    "    _var_names = [\"alpha\", \"phi\", \"nu\"]\n",
    "    model_diagnose(model_covid_AR, trace_covid_AR1, _var_names)\n",
    "    az.plot_trace(trace_covid, var_names=_var_names)\n",
    "    pp_covid_AR = pm.sample_posterior_predictive(trace_covid_AR1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### NUTS vs Metropolis-Hasting\n",
    "The NUTS don't work for our model seting. Based on debugging/error info,\n",
    "our model are stuck where gradient are 0.\n",
    "\n",
    "This naturally motivates use to use the Metropolis-Hasting algorithm,\n",
    "which doesn't rely on gradients.\n",
    "\n",
    "Also, the seminal paper\n",
    "``Stochastic Volatility- Likelihood Inference and Comparison with ARCH Models``\n",
    " from Sangjoon Kim, Neil Shephard and Siddhartha Chib also use MH\n",
    " algorithm.\n",
    "\n",
    "Hence, even with the following cavents from PyMC3 documentations, we\n",
    "change the proposal step from NUTS to Metropolis-Hasting algo.\n",
    "\n",
    "We increase the tuning and sampling periods drasticaly as MH algo is\n",
    "fast.\n",
    "<pre>\n",
    "For almost all continuous models, NUTS should be preferred.\n",
    "\n",
    "There are hard-to-sample models for which NUTS will be very slow causing\n",
    "many users to use Metropolis instead.\n",
    "\n",
    "This practice, however, is rarely successful. NUTS is fast on simple models but can be slow if the model is very complex\n",
    "or it is badly initialized.\n",
    "\n",
    "In the case of a complex model that is hard for NUTS, Metropolis, while faster, will have a very low effective sample\n",
    "size or not converge properly at all.\n",
    "A better approach is to instead try to improve initialization of NUTS, or reparameterize the model.\n",
    "                                                                                        --PyMC3 Documentation\n",
    "</pre>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Code for Free-Scale Stochastic Volatility assumeing a AR log-vol process\n",
    "with model_covid_AR:\n",
    "    # stepNUTS = pm.NUTS(vars=[model_covid_AR.alpha,\n",
    "    #                          model_covid_AR.scale,\n",
    "    #                          model_covid_AR.phi,\n",
    "    #                          model_covid_AR.nu])\n",
    "    # stepMH = pm.Metropolis(vars=[model_covid_AR.log_vol])\n",
    "    # trace_covid_AR1 = pm.sample(8000, tune=8000, step=[stepNUTS,\n",
    "    #                                                stepMH],\n",
    "    #                         return_inferencedata=True)\n",
    "    trace_covid_AR1 = pm.sample(8000, tune=8000, step=pm.Metropolis(),\n",
    "                            return_inferencedata=True)\n",
    "    _var_names = [\"alpha\", \"nu\", \"phi\"]\n",
    "    model_diagnose(model_covid_AR, trace_covid_AR1, _var_names)\n",
    "    az.plot_trace(trace_covid_AR1, var_names=_var_names)\n",
    "    pp_covid_AR = pm.sample_posterior_predictive(trace_covid_AR1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Metropolis-Hasting algorithm generates decent posterior distributions\n",
    "for $\\alpha$ and $\\nu$, but fail for $\\phi$, also the posterior\n",
    "prediction indicate strong overfitting problem. Our predictions\n",
    "explode and hit the sky in some periods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_plot(data=data_cmodel, trace=trace_covid_AR1, pp=pp_covid_AR, AR=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Mini-conclusion\n",
    "* We propose and test four Bayesian Stochatic Volatility Models,\n",
    "among which the Two-state model perform the best.\n",
    "* Having say best performance, we mean that two-state model is\n",
    "parsimonious and responsive to COVID-19 shock.\n",
    "* The Free-Scale Model is difficult to compute, and also suffer\n",
    "from severe overfitting problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclusion\n",
    "Motivated by the stylized fact that VIX and COVID-19 infection number\n",
    "peak simultaneously, we try to model the co-movement using frequentist\n",
    "and bayesian methods.\n",
    "\n",
    "* Frequentist method - The ARCH model fail to incorporate the\n",
    "COVID-19 shock.\n",
    "* Bayesian method - Wepropose and test four models, among\n",
    "which a state-space representation model yield the best performance.\n",
    "* We think our results and discoveries related to COVID-19 is\n",
    "transferable to other events. We can model and gain posterior distributions\n",
    "for any other sentiment or volatility shocks by state-space\n",
    "representation and Monte-Carlo Markov Chain simulation.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}